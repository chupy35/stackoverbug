28993691
How to speed up refreshes with eager global ordinals
<p>I've a 8 nodes cluster (8 r3.xlarge on AWS using the ephemeral 80GB SSD disk coming with the instances). with one main index and two main index types.&#xA;There is a parent/child relationship between them. There are 75M parents and 15M childs. (we cleanup the childs periodically but keep the parents)</p>&#xA;&#xA;<p>In order to speedup the has_child queries, I'm using eager global ordinals on the child's parent field.</p>&#xA;&#xA;<p>Using eager global ordinals seems to significantly slow down the refresh time. A refresh can now take up to 3 seconds.</p>&#xA;&#xA;<p>I believe I can speedup the refreshes by increasing the IO on my machines. But is there any other settings I could play with to fasten the refreshes?</p>&#xA;&#xA;<p>I'm using elasticsearch 1.4.2.</p>&#xA;&#xA;<p>Note that the refreshes were quicker when I was using 8 m3.xlarge with 1 EBS volume. (which doesn't make sense since EBS volumes are supposed to have slower IOs than ephemeral disks...)</p>&#xA;&#xA;<p>Thanks!</p>&#xA;
<p>Without any metrics to point out where the contention might be, it's tricky.  Are you truly IO-bound?  Or compute-bound?  I find more often than naught that I'm the latter.  That's probably your conclusion from comparing two four-core instance types, but other things probably changed as well when you got to the r3.</p>&#xA;&#xA;<p>A few things that may help:</p>&#xA;&#xA;<ul>&#xA;<li><p>Remove merge throttling, <code>index.store.throttle.type: none</code>. With SSDs you don't need it, we just have it on for safety so indexing doesn't consume a box. You can set it per-index, but if testing reveals that it helps and doesn't slow down searches, just throw it in your config on every node, or in your application when you create an index.</p></li>&#xA;<li><p>If you're using Linux, make sure the IO scheduler for your data volume is set to <code>noop</code>, or at least <code>deadline</code>. CFQ is supposed to DTRT when using SSDs but I haven't found that to be true. Besides, on EC2 you don't need a scheduler with any storage (in theory) with the virtualization layer. Keep in mind this won't really make a difference unless you're seeing writes consistently cap out around 60mb/s, or if you're compute-bound.</p></li>&#xA;<li><p>Since you weren't using eager loading on the m3, it's really apples and oranges. Just a hunch, but the slowdown is probably cascading a bit, segments are piling up, and only recover when there's a break in indexing traffic. You could try increasing <code>index.refresh_interval</code> to <code>5s</code> and see if a manual refresh speeds up. This is easy to monitor with <code>/_cat/segments/INDEX | wc -l</code>.</p></li>&#xA;<li><p>You could try increasing the number of threads ES allows Lucene to use. On four cores you probably don't have much headroom (need those CPU graphs), but you could do some tests and check it. <code>index.merge.scheduler.max_thread_count: 6</code> (defaults to 4 on your hardware).</p></li>&#xA;</ul>&#xA;