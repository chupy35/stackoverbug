30517904
Elasticsearch exact matches on analyzed fields
<p>Is there a way to have ElasticSearch identify exact matches on analyzed fields? Ideally, I would like to lowercase, tokenize, stem and perhaps even phoneticize my docs, then have queries pull "exact" matches out.</p>&#xA;&#xA;<p>What I mean is that if I index "Hamburger Buns" and "Hamburgers", they will be analyzed as ["hamburger","bun"] and ["hamburger"]. If I search for "Hamburger", it will only return the "hamburger" doc, as that's the "exact" match. </p>&#xA;&#xA;<p>I've tried using the keyword tokenizer, but that won't stem the individual tokens. Do I need to do something to ensure that the number of tokens is equal or so?</p>&#xA;&#xA;<p>I'm familiar with multi-fields and using the "not_analyzed" type, but this is more restrictive than I'm looking for. I'd like exact matching, post-analysis.</p>&#xA;
<p>You can use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html#_multi_fields_3" rel="nofollow">multi-fields</a> for that purpose and have a <code>not_analyzed</code> sub-field within your <code>analyzed</code> field (let's call it <code>item</code> in this example). Your mapping would have to look like this:</p>&#xA;&#xA;<pre><code>{&#xA;  "yourtype": {&#xA;    "properties": {&#xA;      "item": {&#xA;        "type": "string",&#xA;        "fields": {&#xA;          "raw": {&#xA;            "type": "string",&#xA;            "index": "not_analyzed"&#xA;          }&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>With this kind of mapping, you can check how each of the values <code>Hamburgers</code> and <code>Hamburger Buns</code> are "viewed" by the analyzer with respect to your multi-field <code>item</code> and <code>item.raw</code></p>&#xA;&#xA;<p>For <code>Hamburger</code>:</p>&#xA;&#xA;<pre><code>curl -XGET 'localhost:9200/yourtypes/_analyze?field=item&amp;pretty' -d 'Hamburger'&#xA;{&#xA;  "tokens" : [ {&#xA;    "token" : "hamburger",&#xA;    "start_offset" : 0,&#xA;    "end_offset" : 10,&#xA;    "type" : "&lt;ALPHANUM&gt;",&#xA;    "position" : 1&#xA;  } ]&#xA;}&#xA;curl -XGET 'localhost:9200/yourtypes/_analyze?field=item.raw&amp;pretty' -d 'Hamburger'&#xA;{&#xA;  "tokens" : [ {&#xA;    "token" : "Hamburger",&#xA;    "start_offset" : 0,&#xA;    "end_offset" : 10,&#xA;    "type" : "word",&#xA;    "position" : 1&#xA;  } ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>For <code>Hamburger Buns</code>:</p>&#xA;&#xA;<pre><code>curl -XGET 'localhost:9200/yourtypes/_analyze?field=item&amp;pretty' -d 'Hamburger Buns'&#xA;{&#xA;  "tokens" : [ {&#xA;    "token" : "hamburger",&#xA;    "start_offset" : 0,&#xA;    "end_offset" : 10,&#xA;    "type" : "&lt;ALPHANUM&gt;",&#xA;    "position" : 1&#xA;  }, {&#xA;    "token" : "buns",&#xA;    "start_offset" : 11,&#xA;    "end_offset" : 15,&#xA;    "type" : "&lt;ALPHANUM&gt;",&#xA;    "position" : 2&#xA;  } ]&#xA;}&#xA;curl -XGET 'localhost:9200/yourtypes/_analyze?field=item.raw&amp;pretty' -d 'Hamburger Buns'&#xA;{&#xA;  "tokens" : [ {&#xA;    "token" : "Hamburger Buns",&#xA;    "start_offset" : 0,&#xA;    "end_offset" : 15,&#xA;    "type" : "word",&#xA;    "position" : 1&#xA;  } ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>As you can see, the <code>not_analyzed</code> field is going to be indexed untouched exactly as it was input.</p>&#xA;&#xA;<p>Now, let's index two sample documents to illustrate this:</p>&#xA;&#xA;<pre><code>curl -XPOST localhost:9200/yourtypes/_bulk -d '&#xA;{"index": {"_type": "yourtype", "_id": 1}}&#xA;{"item": "Hamburger"}&#xA;{"index": {"_type": "yourtype", "_id": 2}}&#xA;{"item": "Hamburger Buns"}&#xA;'&#xA;</code></pre>&#xA;&#xA;<p>And finally, to answer your question, if you want to have an exact match on <code>Hamburger</code>, you can search within your sub-field <code>item.raw</code> like this (note that the case has to match, too): </p>&#xA;&#xA;<pre><code>curl -XPOST localhost:9200/yourtypes/yourtype/_search -d '{&#xA;  "query": {&#xA;    "term": {&#xA;      "item.raw": "Hamburger"&#xA;    }&#xA;  }&#xA;}'&#xA;</code></pre>&#xA;&#xA;<p>And you'll get:</p>&#xA;&#xA;<pre><code>{&#xA;  ...&#xA;  "hits" : {&#xA;    "total" : 1,&#xA;    "max_score" : 0.30685282,&#xA;    "hits" : [ {&#xA;      "_index" : "yourtypes",&#xA;      "_type" : "yourtype",&#xA;      "_id" : "1",&#xA;      "_score" : 0.30685282,&#xA;      "_source":{"item": "Hamburger"}&#xA;    } ]&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p><strong>UPDATE (see comments/discussion below and question re-edit)</strong></p>&#xA;&#xA;<p>Taking your example from the comments and trying to have <code>HaMbUrGeR BuNs</code> match <code>Hamburger buns</code> you could simply achieve it with a <code>match</code> query like this.</p>&#xA;&#xA;<pre><code>curl -XPOST localhost:9200/yourtypes/yourtype/_search?pretty -d '{&#xA;  "query": {&#xA;    "match": {&#xA;      "item": {&#xA;        "query": "HaMbUrGeR BuNs",&#xA;        "operator": "and"&#xA;      }&#xA;    }&#xA;  }&#xA;}'&#xA;</code></pre>&#xA;&#xA;<p>Which based on the same two indexed documents above will yield</p>&#xA;&#xA;<pre><code>{&#xA;  ...&#xA;  "hits" : {&#xA;    "total" : 1,&#xA;    "max_score" : 0.2712221,&#xA;    "hits" : [ {&#xA;      "_index" : "yourtypes",&#xA;      "_type" : "yourtype",&#xA;      "_id" : "2",&#xA;      "_score" : 0.2712221,&#xA;      "_source":{"item": "Hamburger Buns"}&#xA;    } ]&#xA;  }&#xA;}&#xA;</code></pre>&#xA;
<p>You can keep the analyzer as what you expected (lowercase, tokenize, stem, ...), and use <code>query_string</code> as the main query, <code>match_phrase</code> as the boosting query to search. Something like this: </p>&#xA;&#xA;<pre><code>{&#xA;   "bool" : {&#xA;      "should" : [&#xA;         {&#xA;            "query_string" : {&#xA;               "default_field" : "your_field",&#xA;               "default_operator" : "OR",&#xA;               "phrase_slop" : 1,&#xA;               "query" : "Hamburger"&#xA;            }&#xA;         },&#xA;         {&#xA;            "match_phrase": {&#xA;               "your_field": {&#xA;                  "query": "Hamburger"&#xA;               }&#xA;            }&#xA;         }&#xA;      ]&#xA;   }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>It will match both documents, and exact match (match_phrase) will be on top since the query match both <code>should</code> clauses (and get higher score)</p>&#xA;&#xA;<p><code>default_operator</code> is set to OR, it will help the query "Hamburger Buns" (match <code>hamburger</code> OR <code>bun</code>) match the document "Hamburger" also.&#xA;<code>phrase_slop</code> is set to 1 to match terms with distance = 1 only, e.g. search for <code>Hamburger Buns</code> will not match document <code>Hamburger Big Buns</code>. You can adjust this depend on your requirements.</p>&#xA;&#xA;<p>You can refer <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/_closer_is_better.html" rel="noreferrer">Closer is better</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/1.x/query-dsl-query-string-query.html" rel="noreferrer">Query string</a> for more details.</p>&#xA;
<p>Use shingles tokenizer together with stemming and whatever else you need. Add a sub-field of type <code>token_count</code> that will count the number of tokens in the field.</p>&#xA;&#xA;<p>At searching time, you need to add an additional filter to match the number of tokens in the index with the number of tokens you have in the searching text. You would need an additional step, when you perform the actual search, that should count the tokens in the searching string. This is like this because shingles will create multiple permutations of tokens and you need to make sure that it matches the size of your searching text.</p>&#xA;&#xA;<p>An attempt for this, just to give you an idea:</p>&#xA;&#xA;<pre><code>{&#xA;  "settings": {&#xA;    "analysis": {&#xA;      "filter": {&#xA;        "filter_shingle": {&#xA;          "type": "shingle",&#xA;          "max_shingle_size": 10,&#xA;          "min_shingle_size": 2,&#xA;          "output_unigrams": true&#xA;        },&#xA;        "filter_stemmer": {&#xA;          "type": "porter_stem",&#xA;          "language": "_english_"&#xA;        }&#xA;      },&#xA;      "analyzer": {&#xA;        "ShingleAnalyzer": {&#xA;          "tokenizer": "standard",&#xA;          "filter": [&#xA;            "lowercase",&#xA;            "snowball",&#xA;            "filter_stemmer",&#xA;            "filter_shingle"&#xA;          ]&#xA;        }&#xA;      }&#xA;    }&#xA;  },&#xA;  "mappings": {&#xA;    "test": {&#xA;      "properties": {&#xA;        "text": {&#xA;          "type": "string",&#xA;          "analyzer": "ShingleAnalyzer",&#xA;          "fields": {&#xA;            "word_count": {&#xA;              "type": "token_count",&#xA;              "store": "yes",&#xA;              "analyzer": "ShingleAnalyzer"&#xA;            }&#xA;          }&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>And the query:</p>&#xA;&#xA;<pre><code>{&#xA;  "query": {&#xA;    "filtered": {&#xA;      "query": {&#xA;        "match_phrase": {&#xA;          "text": {&#xA;            "query": "HaMbUrGeRs BUN"&#xA;          }&#xA;        }&#xA;      },&#xA;      "filter": {&#xA;        "term": {&#xA;          "text.word_count": "2"&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>The <code>shingles</code> filter is important here because it can create combinations of tokens. And more than that, these are combinations that keep the order or the tokens. Imo, the most difficult requirement to fulfill here is to change the tokens (stemming, lowercasing etc) and, also, to assemble back the original text. Unless you define your own "concatenation" filter I don't think there is any other way than using the <code>shingles</code> filter.</p>&#xA;&#xA;<p>But with <code>shingles</code> there is another issue: it creates combinations that are not needed. For a text like <code>"Hamburgers buns in Los Angeles"</code> you end up with a long list of shingles:</p>&#xA;&#xA;<pre><code>          "angeles",&#xA;          "buns",&#xA;          "buns in",&#xA;          "buns in los",&#xA;          "buns in los angeles",&#xA;          "hamburgers",&#xA;          "hamburgers buns",&#xA;          "hamburgers buns in",&#xA;          "hamburgers buns in los",&#xA;          "hamburgers buns in los angeles",&#xA;          "in",&#xA;          "in los",&#xA;          "in los angeles",&#xA;          "los",&#xA;          "los angeles"&#xA;</code></pre>&#xA;&#xA;<p>If you are interested in only those documents that match <strong>exactly</strong> meaning, the documents above matches only when you search for "hamburgers buns in los angeles" (and doesn't match something like "any hamburgers buns in los angeles") then you need a way to filter that long list of shingles. The way I see it is to  use <code>word_count</code>.</p>&#xA;