33896243
Bulk load Insert in Elasticsearch with large volume
<p>I have 10GB data, that i need to load into elastic-search index, and I have converted the data to JSON formatted.&#xA;My question is when i try to load all the data into elastic search using CRUL command its thrown below error.&#xA;at the same time when I split the JSON files to multiple 1GB files then it works fine.&#xA;Do we need to follow any methodology to load large size file to elasticsearch or any predefined tools are available ? Please advise here !</p>&#xA;&#xA;<p>Full File(10GB)</p>&#xA;&#xA;<pre><code>curl -XPOST 'http://servername:9200/xyz/tmp/_bulk?pretty' --data-binary @/home/xyz/test.json&#xA;</code></pre>&#xA;&#xA;<p>Error</p>&#xA;&#xA;<pre><code>curl: (56) Failure when receiving data from the peer&#xA;</code></pre>&#xA;&#xA;<p>Split File(Success command)</p>&#xA;&#xA;<pre><code>curl -XPOST 'http://servername:9200/xyz/tmp/_bulk?pretty' --data-binary @/home/xyz/test_split1.json&#xA;curl -XPOST 'http://servername:9200/xyz/tmp/_bulk?pretty' --data-binary @/home/xyz/test_split2.json&#xA;</code></pre>&#xA;
<p>There is a http request size limit of <code>Integer.MAX_VALUE</code> or <code>2^31-1</code> which is basically <strong>2GB</strong>. </p>&#xA;&#xA;<p>If you check your ES logs you would see something like this <code>HTTP content length exceeded 104857600 bytes</code> and hence you can not index 10GB data at once, you have to split the file.</p>&#xA;&#xA;<p>Please refer to <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-http.html" rel="nofollow noreferrer">docs</a>. Also <a href="https://stackoverflow.com/questions/28841221/what-is-the-maximum-elasticsearch-document-size">this answer</a> would help a lot</p>&#xA;