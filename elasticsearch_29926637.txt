29926637
Elasticsearch / Kibana field data too large
<p>I have a small ELK cluster that is in testing. The kibana web interface is extremely slow and throws a lot of errors. </p>&#xA;&#xA;<p>Kafka => 8.2<br>&#xA;Logstash => 1.5rc3  (latest)<br>&#xA;Elasticsearch => 1.4.4 (latest)<br>&#xA;Kibana => 4.0.2 (latest)  </p>&#xA;&#xA;<p>The elasticsearch nodes have 10GB of ram each on Ubuntu 14.04. I'm pulling in between 5GB and 20GB of data per day. </p>&#xA;&#xA;<p>Running even a simple query, with only 15 minutes of data in the kibana web interface takes several minutes, and often throws errors. </p>&#xA;&#xA;<pre><code>[FIELDDATA] Data too large, data for [timeStamp] would be larger than limit of [3751437926/3.4gb]]&#xA;</code></pre>&#xA;&#xA;<p><img src="https://i.stack.imgur.com/EdVOB.png" alt="enter image description here"></p>&#xA;&#xA;<p>These errors about the shard failures only appear in kibana. According to all other plugins(head, kopf), the elasticsearch shards are perfectly fine, and the cluster is green.  </p>&#xA;&#xA;<p>I've checked with the google group, IRC and looked at stack overflow. It seems the only solution is to increase the ram. I've increased the ram on my nodes twice. While that seems to fix it for a day or two, the problem quickly returns. &#xA;Other solutions such as cleaning the cache have no long term improvements. </p>&#xA;&#xA;<pre><code>curl -XPUT 'http://elastic.example.com:9200/cache/clear?filter=true'&#xA;curl -XPOST 'http://elastic.example.com:9200/_cache/clear' -d '{ "fielddata": "true" }'&#xA;</code></pre>&#xA;&#xA;<p>According to the KOPF plugin, the amount of heap space routinely approaches 75% on a completely idle cluster. (I'm the only one in the company using it). 3 Nodes with 10GB of ram should be more than enough for the amount of data that I have. </p>&#xA;&#xA;<p>I have also tried adjusting the breakers as <a href="http://evertrue.github.io/blog/2014/11/16/3-performance-tuning-tips-for-elasticsearch/" rel="nofollow noreferrer">suggested by this blog.</a> </p>&#xA;&#xA;<pre><code>PUT /_cluster/settings -d '{ "persistent" : { "indices.breaker.fielddata.limit" : "70%" } }'&#xA;PUT /_cluster/settings -d '{ "persistent" : {  "indices.fielddata.cache.size" : "60%" } }'&#xA;</code></pre>&#xA;&#xA;<p>How can I prevent these errors , and fix the extreme slowness in kibana? </p>&#xA;&#xA;<p><a href="https://github.com/elastic/kibana/issues/3221" rel="nofollow noreferrer">https://github.com/elastic/kibana/issues/3221</a><br>&#xA;<a href="https://stackoverflow.com/questions/27296340/elasticsearch-getting-too-many-results-need-help-filtering-query">elasticsearch getting too many results, need help filtering query</a><br>&#xA;<a href="http://elasticsearch-users.115913.n3.nabble.com/Data-too-large-error-td4060962.html" rel="nofollow noreferrer">http://elasticsearch-users.115913.n3.nabble.com/Data-too-large-error-td4060962.html</a>   </p>&#xA;&#xA;<p><strong>Update</strong></p>&#xA;&#xA;<p>I have about 30 days of indexes from logstash. 2x Replication so that is 10 shards per day. </p>&#xA;&#xA;<p><img src="https://i.stack.imgur.com/rjNmp.png" alt="enter image description here"></p>&#xA;&#xA;<p><strong>Update2</strong></p>&#xA;&#xA;<p>I've increased the ram of each node to 16GB, (48GB total) and I've also upgraded to 1.5.2.</p>&#xA;&#xA;<p><img src="https://i.stack.imgur.com/Qojt8.png" alt="enter image description here"></p>&#xA;&#xA;<p>This appears to fix the issue for a day or two, however the problem returns. </p>&#xA;&#xA;<p><img src="https://i.stack.imgur.com/J45rD.png" alt="enter image description here"></p>&#xA;&#xA;<p><strong>Update3</strong></p>&#xA;&#xA;<p><a href="https://www.elastic.co/blog/support-in-the-wild-my-biggest-elasticsearch-problem-at-scale" rel="nofollow noreferrer">This blog article from an elastic employee has good tips</a> explaining what can cause these issues. </p>&#xA;
<p>The basic ideas would include:</p>&#xA;&#xA;<ul>&#xA;<li>Fewer open indexes.</li>&#xA;<li>Fewer shards.</li>&#xA;<li>Use of doc_values.</li>&#xA;</ul>&#xA;&#xA;<p>Oh, and:</p>&#xA;&#xA;<ul>&#xA;<li>More RAM.</li>&#xA;</ul>&#xA;
<p>You're indexing a lot of data (if you're adding/creating 5 to 20GB a day) and your nodes are quite low on memory. You won't see any problems on the indexing front but fetching data on a single or multiple indexes will cause problems. Keep in mind that Kibana runs queries in the background and the message you're getting is basically saying something along the lines of "<em>I can't get that data for you because I need to put more data in memory than I have available in order to run these queries.</em>" </p>&#xA;&#xA;<p>There are two things that are relatively simple to do and should solve your problems:</p>&#xA;&#xA;<ul>&#xA;<li>Upgrade to <a href="https://www.elastic.co/downloads/past-releases/elasticsearch-1-5-2">ElasticSearch 1.5.2</a> (Major performance improvements)</li>&#xA;<li>When you're short on memory, you really need to use <a href="http://www.elastic.co/guide/en/elasticsearch/guide/current/doc-values.html">doc_values</a> in all of your mappings as this will reduce the heap size drastically</li>&#xA;</ul>&#xA;&#xA;<p>The key lies in <strong>doc_values</strong> though. You need to modify your mapping(s) to set this property to <em>true</em>. Crude example:</p>&#xA;&#xA;<pre><code>[...],&#xA;"properties": {&#xA;    "age": {&#xA;      "type": "integer",&#xA;      "doc_values": true&#xA;    },&#xA;    "zipcode": {&#xA;      "type": "integer",&#xA;      "doc_values": true&#xA;    },&#xA;    "nationality": {&#xA;      "type": "string",&#xA;      "index": "not_analyzed",&#xA;      "doc_values": true&#xA;    },&#xA;    [...]&#xA;</code></pre>&#xA;&#xA;<p>Updating your mapping(s) will make future indexes take this into account but you'll need to reindex existing ones entirely for <em>doc_values</em> to apply on existing indexes. (See <a href="http://www.elastic.co/guide/en/elasticsearch/guide/master/scan-scroll.html">scan/scroll</a> and <a href="https://www.elastic.co/blog/changing-mapping-with-zero-downtime">this</a> blog post for more tips.)</p>&#xA;&#xA;<p>Replicas help scale but will run into the same problems if you don't reduce the heap size of each node. As for the number of shards you currently have, it may not be necessary nor optimal but I don't think it's the root cause of your problems.</p>&#xA;&#xA;<p>Keep in mind that the suggestions mentioned above are to allow Kibana to run the queries and show you data. Speed will rely greatly on the date ranges you set, on the machines you have (CPU, SSD, etc), and on the memory available on each node.</p>&#xA;