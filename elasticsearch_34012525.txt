34012525
Nodes loading, but Elasticsearch has 0 shards
<p>I was testing out a 20 node cluster with default replicates, default sharding, and recently wanted to rename the cluster from the default "elasticsearch."  So, I updated the config cluster name, and additionally renamed the data from </p>&#xA;&#xA;<pre><code>mylocation/data/OldName&#xA;</code></pre>&#xA;&#xA;<p>to</p>&#xA;&#xA;<pre><code>mylocation/data/NewName&#xA;</code></pre>&#xA;&#xA;<p>Which of course contain:</p>&#xA;&#xA;<pre><code>nodes/0&#xA;nodes/1&#xA;</code></pre>&#xA;&#xA;<p>etc...</p>&#xA;&#xA;<p>About a month later, I'm loading up my cluster again, and I see that while all 20 nodes come back online, it says 0 active shards, 0 primary shards, etc. where this should be several thousand.  Status is green, nothing is initializing, nothing looks amiss except I have no data.  I look in <code>nodes/0</code> and I see <code>nodes/0/indices/</code> are well populated with my index names: the data is actually on the disk.  But it seems there's nothing I can do to get it to actually load the shards.  The config is using the correct <code>Des.path.data=mylocation/data/</code>.</p>&#xA;&#xA;<p>What could be wrong and how can I debug it?  I'm fairly confident I ran this for a week after loading it, but it was some time ago and perhaps other things have changed.  It just oddly seems to not be recognize any of the data it's pointing at, and it isn't giving me any kind of "I don't see your data" or "cannot read or write that data" error message.</p>&#xA;&#xA;<p><em>Update</em></p>&#xA;&#xA;<p>After it gets started it says:</p>&#xA;&#xA;<p>Recovered [0] indices into cluster_state.</p>&#xA;&#xA;<p>I googled this and it sounded like version compatibility. Checked my binaries and this did not appear to be an issue. I'm using 1.3.2 on all.</p>&#xA;&#xA;<p><em>Update 2</em></p>&#xA;&#xA;<p>One of 20 nodes repeatly fails with &#xA;ElasticsearchillegalStateException[failed to obtain node lock, is the following location writable?]</p>&#xA;&#xA;<p>It lists the correct data dir, which is writable. Should I delete the node lock? Some node.locks are 664 and some are 640 when the cluster is off. Is this normal or possibly the relic of an unclean shutdown?</p>&#xA;&#xA;<p>Are some of these replicates? I have 40 nodes, 20 are 640 and 20 are 664.</p>&#xA;&#xA;<p><em>Update 3</em></p>&#xA;&#xA;<p>There are write locks in place at the lucene level. So </p>&#xA;&#xA;<p>data/NewName/nodes/1/indices/indexname/4/index/write.lock</p>&#xA;&#xA;<p>exists. Is this why moving shards fails? Can i safely delete each of these write locks or is there shared state in the _state file that would lead to inconsistency?</p>&#xA;