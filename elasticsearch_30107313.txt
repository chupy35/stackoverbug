30107313
ElasticSearch Date Field Mapping Malformation
<p>In my <strong>ElasticHQ mapping</strong>:</p>&#xA;&#xA;<pre><code>@timestamp  date    yyyy-MM-dd HH:mm:ssZZZ&#xA;...&#xA;date    date    yyyy-MM-dd HH:mm:ssZZZ&#xA;</code></pre>&#xA;&#xA;<p>In the above I have two types of date field each with a mapping to the same format.</p>&#xA;&#xA;<p>In the <strong>data</strong>:</p>&#xA;&#xA;<pre><code>"@timestamp": "2014-05-21 23:22:47UTC"&#xA;....&#xA;"date": "2014-05-22 05:08:09-0400",&#xA;</code></pre>&#xA;&#xA;<p>As above, the date format does not map to what ES thinks I have my dates formatted as. I assume something hinky happened at index time (I wasn't around).</p>&#xA;&#xA;<p>Also interesting: When using a filtered range query like the following, I get a Parsing Exception explaining that my date is too short:</p>&#xA;&#xA;<pre><code>GET _search&#xA;{&#xA;   "query": {&#xA;       "filtered": {&#xA;          "query": {&#xA;            "match_all": {}&#xA;          },&#xA;          "filter": {&#xA;              "range": {&#xA;                 "date": {&#xA;                    "from": "2013-11-23 07:00:29",&#xA;                    "to": "2015-11-23 07:00:29",&#xA;                    "time_zone": "+04:00"&#xA;                 }&#xA;              }&#xA;          }&#xA;       }&#xA;   }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>But searching with the following passes ES's error check, but returns no results, I assume because of the date formatting in the documents.</p>&#xA;&#xA;<pre><code>    GET _search&#xA;{&#xA;   "query": {&#xA;       "filtered": {&#xA;          "query": {&#xA;            "match_all": {}&#xA;          },&#xA;          "filter": {&#xA;              "range": {&#xA;                 "date": {&#xA;                    "from": "2013-11-23 07:00:29UTC",&#xA;                    "to": "2015-11-23 07:00:29UTC",&#xA;                    "time_zone": "+04:00"&#xA;                 }&#xA;              }&#xA;          }&#xA;       }&#xA;   }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>My question is this: given the above, is there any way we can avoid having to Re-Index and change the mapping and continue to search the malformed data? WE have around 1TB of data in this particular cluster, and would like to keep it as is, for obvious reasons.</p>&#xA;&#xA;<p>Also attempted was a query that adheres to what is in the Data:</p>&#xA;&#xA;<pre><code>"query": {&#xA;   "range": {&#xA;      "date": {&#xA;         "gte": "2014-05-22 05:08:09-0400",&#xA;         "to": "2015-05-22 05:08:09-0400"&#xA;      }&#xA;   }&#xA;}&#xA;</code></pre>&#xA;
<p>The dates you have in your documents actually do conform to the date format you have in your mapping, i.e. <code>yyyy-MM-dd HH:mm:ssZZZ</code></p>&#xA;&#xA;<p>In date format patterns, <code>ZZZ</code> stands for an RFC 822 time zone (e.g. -04:00, +04:00, EST, UTC, GMT, ...) so the dates you have in your data do comply otherwise they wouldn't have been indexed in the first place.</p>&#xA;&#xA;<p>However, the best practice is to always make sure dates are transformed to UTC (or any other time zone common to the whole document base that makes sense in your context) <strong>before indexing them</strong> so that you have a common basis to query on. </p>&#xA;&#xA;<p>As for your query that triggers errors, <code>2013-11-23 07:00:29</code> doesn't comply with the date format since the time zone is missing at the end. As you've rightly discovered, adding <code>UTC</code> at the end fixes the query parsing problem (i.e. the missing <code>ZZZ</code> part), but you might still get no results.</p>&#xA;&#xA;<p>Now to answer your question, you have two main tasks to do:</p>&#xA;&#xA;<ol>&#xA;<li>Fix your indexing process/component to make sure all the dates are in a common timezone (usually UTC)</li>&#xA;<li>Fix your existing data to transform the dates in your indexed documents into the same timezone</li>&#xA;</ol>&#xA;&#xA;<p>1TB is a lot of data to reindex for fixing one or two fields. I don't know how your documents look like, but it doesn't really matter. The way I would approach the problem would be to run a partial update on all documents, and for this, I see two different solutions, in both of which the idea is to just fix the <code>@timestamp</code> and <code>date</code> fields:</p>&#xA;&#xA;<ol>&#xA;<li>Depending on your version of ES, you can use the <a href="https://github.com/yakaz/elasticsearch-action-updatebyquery" rel="nofollow">update-by-query plugin</a> but transforming a date via script is a bit cumbersome.</li>&#xA;<li>Or you can write an adhoc client that will <a href="http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-scroll.html#scroll-scan" rel="nofollow">scroll</a> over all your existing documents and <a href="http://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html#_using_scripts_to_make_partial_updates" rel="nofollow">partial update</a> each of them and send them back <a href="http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html" rel="nofollow">in bulk</a>.</li>&#xA;</ol>&#xA;&#xA;<p>Given the amount of data you have, solution 2 seems more appropriate. </p>&#xA;&#xA;<p>So... your adhoc script should first issue a scroll query to obtain a scroll id like this:</p>&#xA;&#xA;<pre><code>curl -XGET 'server:9200/your_index/_search?search_type=scan&amp;scroll=1m' -d '{&#xA;    "query": { "match_all": {}},&#xA;    "size":  1000&#xA;}'&#xA;</code></pre>&#xA;&#xA;<p>As a result, you'll get a scroll id that you can now use to iterate over all your data with</p>&#xA;&#xA;<pre><code>curl -XGET 'server:9200/_search/scroll?_source=date,@timestamp&amp;scroll=1m' -d 'your_scroll_id'&#xA;</code></pre>&#xA;&#xA;<p>You'll get 1000 hits (you can de/increase the <code>size</code> parameter in the first query above depending on your mileage) that you can now iterate over.</p>&#xA;&#xA;<p>For each hit you get, you'll only have your two date fields that you need to fix. Then you can transform your dates into the standard timezone of your choosing using a <a href="http://javacodingtutorial.blogspot.ch/2014/07/convert-time-to-any-timezone.html" rel="nofollow">solution like this</a> for instance.</p>&#xA;&#xA;<p>Finally, you can send your 1000 updated partial documents in one bulk like this:</p>&#xA;&#xA;<pre><code>curl -XPOST server:9200/_bulk -d '&#xA;{ "update" : {"_id" : "1", "_type" : "your_type", "_index" : "your_index"} }&#xA;{ "doc" : {"date" : "2013-11-23 07:00:29Z", "@timestamp": "2013-11-23 07:00:29Z"} }&#xA;{ "update" : {"_id" : "2", "_type" : "your_type", "_index" : "your_index"} }&#xA;{ "doc" : {"date" : "2014-09-12 06:00:29Z", "@timestamp": "2014-09-12 06:00:29Z"} }&#xA;...&#xA;'&#xA;</code></pre>&#xA;&#xA;<p>Rinse and repeat with the next iteration...</p>&#xA;&#xA;<p>I hope this should give you some initial pointers to get started. Let us know if you have any questions.</p>&#xA;