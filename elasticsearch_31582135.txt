31582135
Combining my tokenizer with synonym token filter
<p>I wrote my own tokenizer which splits the text on ' ' (space) and if I have Specific pre-defined phrases, they don't split. For example 'meat free' is a phrase and it won't be split.&#xA;So my tokenizer works well but when I use it in an analyzer in combination with synonym token filter it does not replace 'meat free' with its synonyms.</p>&#xA;&#xA;<p>Do I have a way to make it work without re-writing my own synonym token filter? </p>&#xA;&#xA;<p><strong>This is how I set my anlyzer:</strong></p>&#xA;&#xA;<pre><code> "settings": {&#xA;        "index" : {&#xA;            "analysis" : { &#xA;                "analyzer" : {&#xA;                    "my_analyzer" : {&#xA;                        "tokenizer" : "phrase_tokenizer",&#xA;                        "filter" : ["lowercase", "synonym"]&#xA;                    }&#xA;                },&#xA;                "filter" : {&#xA;                    "synonym" : {&#xA;                        "type" : "synonym",&#xA;                        "synonyms" : [&#xA;                            "meat free=&gt; vegan, vegetarian, veggie"&#xA;                        ]&#xA;                    }&#xA;                }&#xA;            }&#xA;        }&#xA;    }, &#xA;</code></pre>&#xA;&#xA;<p><strong>This is how I test it:</strong></p>&#xA;&#xA;<pre><code>GET /my_index/_analyze?analyzer=my_analyzer&amp;text=try try meat free try try&#xA;</code></pre>&#xA;&#xA;<p>Result tokens are: <code>[try,try,meat free,try,try]</code></p>&#xA;&#xA;<p>I expect to get:  <code>[try,try,vegan,vegetarian,veggiee,try,try]</code></p>&#xA;