30574868
Elasticsearch: Remove duplicates from index
<p>I have an index with multiple duplicate entries. They have different ids but the other fields have identical content.</p>&#xA;&#xA;<p>For example:</p>&#xA;&#xA;<pre><code>{id: 1, content: 'content1'}&#xA;{id: 2, content: 'content1'}&#xA;{id: 3, content: 'content2'}&#xA;{id: 4, content: 'content2'}&#xA;</code></pre>&#xA;&#xA;<p>After removing the duplicates:</p>&#xA;&#xA;<pre><code>{id: 1, content: 'content1'}&#xA;{id: 3, content: 'content2'}&#xA;</code></pre>&#xA;&#xA;<p>Is there a way to delete all duplicates and keep only one distinct entry without manually comparing all entries?</p>&#xA;
<p>I use rails and if necessary I will import things with the <code>FORCE=y</code> command, which removes and re-indexes everything for that index and type... however not sure what environment you are running ES in. Only issue I can see is if the data source you are importing from (i.e. Database) has duplicate records. I guess I would see first if the data source could be fixed, if that is feasible, and you re-index everything; otherwise you could try to create a custom import method that only indexes one of the duplicate items for each record.</p>&#xA;&#xA;<p>Furthermore, and I know this doesn't comply with you wanting to remove duplicate entries, but you could simply customize your search so that you are only returning one of the duplicate ids back, either by most recent "timestamp" or indexing deduplicated data and grouping by your content field -- see if <a href="https://stackoverflow.com/questions/25448186/remove-duplicate-documents-from-a-search-in-elasticsearch">this post helps</a>. Even though this would still retain the duplicate records in your index, at least they won't come up in the search results.</p>&#xA;&#xA;<p>I also found this as well: <a href="https://stackoverflow.com/questions/24839118/elasticsearch-delete-duplicates">Elasticsearch delete duplicates</a></p>&#xA;&#xA;<p>I tried thinking of many possible scenarios for you to see if any of those options work or at least could be a temp fix.</p>&#xA;
<p>This can be accomplished in several ways. Below I outline two possible approaches:</p>&#xA;&#xA;<p>1) If you don't mind generating new <code>_id</code> values and reindexing all of the documents into a new collection, then you can use Logstash and the <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-fingerprint.html" rel="nofollow noreferrer">fingerprint</a> filter to generate a unique fingerprint (hash) from the fields that you are trying to de-duplicate, and use this fingerprint as the <code>_id</code> for documents as they are written into the new collection. Since the <code>_id</code> field must be unique, any documents that have the same fingerprint will be written to the same <code>_id</code> and therefore deduplicated.</p>&#xA;&#xA;<p>2) You can write a custom script that scrolls over your index. As each document is read, you can create a hash from the fields that you consider to define a unique document (in your case, the <code>content</code> field). Then use this hash as they key in a dictionary (aka hash table). The value associated with this key would be a list of all of the document's <code>_id</code>s that generate this same hash. Once you have all of the hashes and associated lists of <code>_id</code>s, you can execute a delete operation on all but one of the <code>_id</code>s that are associated with each identical hash. Note that this second approach does not require writing documents to a new index in order to de-duplicate, as you would delete documents directly from the original index.</p>&#xA;&#xA;<p>I have written a blog post and code that demonstrates both of these approaches at the following URL: <a href="https://alexmarquardt.com/2018/07/23/deduplicating-documents-in-elasticsearch/" rel="nofollow noreferrer">https://alexmarquardt.com/2018/07/23/deduplicating-documents-in-elasticsearch/</a></p>&#xA;&#xA;<p>Disclaimer: I am a Consulting Engineer at Elastic. </p>&#xA;