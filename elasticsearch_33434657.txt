33434657
Why are shards getting initialized and relocated during bulk insert
<p>I'm trying to bulk insert data into a 4 node elasticsearch cluster with 3 data nodes.</p>&#xA;&#xA;<p>Data nodes specs :&#xA;16 CPU - 7GB RAM - 500GB SSD</p>&#xA;&#xA;<p>The data is inserted on the non-data node and split on 5 shards and set to have 1 replicate.&#xA;There is approximately 250GB of data to insert.</p>&#xA;&#xA;<p>However, after ~40GB of data inserted on each node and one hour of processing while having ~60%CPU and ~30%RAM usage maximum during the whole timespan, some shards get in initialized state:</p>&#xA;&#xA;<pre><code>~$ curl -XGET 'http://localhost:9200/_cluster/health/osm?level=shards&amp;pretty=true'&#xA;{&#xA;  "cluster_name" : "elastic_osm",&#xA;  "status" : "yellow",&#xA;  "timed_out" : false,&#xA;  "number_of_nodes" : 4,&#xA;  "number_of_data_nodes" : 3,&#xA;  "active_primary_shards" : 5,&#xA;  "active_shards" : 9,&#xA;  "relocating_shards" : 1,&#xA;  "initializing_shards" : 1,&#xA;  "unassigned_shards" : 0,&#xA;  "delayed_unassigned_shards" : 0,&#xA;  "number_of_pending_tasks" : 0,&#xA;  "number_of_in_flight_fetch" : 0,&#xA;  "indices" : {&#xA;    "osm" : {&#xA;      "status" : "yellow",&#xA;      "number_of_shards" : 5,&#xA;      "number_of_replicas" : 1,&#xA;      "active_primary_shards" : 5,&#xA;      "active_shards" : 9,&#xA;      "relocating_shards" : 1,&#xA;      "initializing_shards" : 1,&#xA;      "unassigned_shards" : 0,&#xA;      "shards" : {&#xA;        "0" : {&#xA;          "status" : "yellow",&#xA;          "primary_active" : true,&#xA;          "active_shards" : 1,&#xA;          "relocating_shards" : 0,&#xA;          "initializing_shards" : 1,&#xA;          "unassigned_shards" : 0&#xA;        },&#xA;        "1" : {&#xA;          "status" : "green",&#xA;          "primary_active" : true,&#xA;          "active_shards" : 2,&#xA;          "relocating_shards" : 0,&#xA;          "initializing_shards" : 0,&#xA;          "unassigned_shards" : 0&#xA;        },&#xA;        "2" : {&#xA;          "status" : "green",&#xA;          "primary_active" : true,&#xA;          "active_shards" : 2,&#xA;          "relocating_shards" : 1,&#xA;          "initializing_shards" : 0,&#xA;          "unassigned_shards" : 0&#xA;        },&#xA;        "3" : {&#xA;          "status" : "green",&#xA;          "primary_active" : true,&#xA;          "active_shards" : 2,&#xA;          "relocating_shards" : 0,&#xA;          "initializing_shards" : 0,&#xA;          "unassigned_shards" : 0&#xA;        },&#xA;        "4" : {&#xA;          "status" : "green",&#xA;          "primary_active" : true,&#xA;          "active_shards" : 2,&#xA;          "relocating_shards" : 0,&#xA;          "initializing_shards" : 0,&#xA;          "unassigned_shards" : 0&#xA;        }&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Digging a bit deeper, I found that one node has a problem with the heap space :</p>&#xA;&#xA;<pre><code>~$ curl -XGET 'localhost:9200/osm/_search_shards?pretty=true'&#xA;{&#xA;  "nodes" : {&#xA;    "1DpvDUf7SKywJrBgQqs9eg" : {&#xA;      "name" : "elastic-osm-node-1",&#xA;      "transport_address" : "inet[/xxx.xxx.x.x:xxxx]",&#xA;      "attributes" : {&#xA;        "master" : "true"&#xA;      }&#xA;    },&#xA;    "FiBYw-v_QfO3nJQfHflf_w" : {&#xA;      "name" : "elastic-osm-node-3",&#xA;      "transport_address" : "inet[/xxx.xxx.x.x:x]",&#xA;      "attributes" : {&#xA;        "master" : "true"&#xA;      }&#xA;    },&#xA;    "ibpt8lGiS6yDJf4e09RN9Q" : {&#xA;      "name" : "elastic-osm-node-2",&#xA;      "transport_address" : "inet[/xxx.xxx.x.x:xxxx]",&#xA;      "attributes" : {&#xA;        "master" : "true"&#xA;      }&#xA;    }&#xA;  },&#xA;  "shards" : [ [ {&#xA;    "state" : "STARTED",&#xA;    "primary" : true,&#xA;    "node" : "ibpt8lGiS6yDJf4e09RN9Q",&#xA;    "relocating_node" : null,&#xA;    "shard" : 0,&#xA;    "index" : "osm"&#xA;  }, {&#xA;    "state" : "INITIALIZING",&#xA;    "primary" : false,&#xA;    "node" : "FiBYw-v_QfO3nJQfHflf_w",&#xA;    "relocating_node" : null,&#xA;    "shard" : 0,&#xA;    "index" : "osm",&#xA;    "unassigned_info" : {&#xA;      "reason" : "ALLOCATION_FAILED",&#xA;      "at" : "2015-10-30T10:42:25.539Z",&#xA;      "details" : "shard failure [engine failure, reason [already closed by tragic event]][OutOfMemoryError[Java heap space]]"&#xA;    }&#xA;  } ], [ {&#xA;    "state" : "STARTED",&#xA;    "primary" : true,&#xA;    "node" : "FiBYw-v_QfO3nJQfHflf_w",&#xA;    "relocating_node" : null,&#xA;    "shard" : 1,&#xA;    "index" : "osm"&#xA;  }, {&#xA;    "state" : "STARTED",&#xA;    "primary" : false,&#xA;    "node" : "1DpvDUf7SKywJrBgQqs9eg",&#xA;    "relocating_node" : null,&#xA;    "shard" : 1,&#xA;    "index" : "osm"&#xA;  } ], [ {&#xA;    "state" : "RELOCATING",&#xA;    "primary" : false,&#xA;    "node" : "FiBYw-v_QfO3nJQfHflf_w",&#xA;    "relocating_node" : "1DpvDUf7SKywJrBgQqs9eg",&#xA;    "shard" : 2,&#xA;    "index" : "osm"&#xA;  }, {&#xA;    "state" : "STARTED",&#xA;    "primary" : true,&#xA;    "node" : "ibpt8lGiS6yDJf4e09RN9Q",&#xA;    "relocating_node" : null,&#xA;    "shard" : 2,&#xA;    "index" : "osm"&#xA;  }, {&#xA;    "state" : "INITIALIZING",&#xA;    "primary" : false,&#xA;    "node" : "1DpvDUf7SKywJrBgQqs9eg",&#xA;    "relocating_node" : "FiBYw-v_QfO3nJQfHflf_w",&#xA;    "shard" : 2,&#xA;    "index" : "osm"&#xA;  } ], [ {&#xA;    "state" : "STARTED",&#xA;    "primary" : false,&#xA;    "node" : "FiBYw-v_QfO3nJQfHflf_w",&#xA;    "relocating_node" : null,&#xA;    "shard" : 3,&#xA;    "index" : "osm"&#xA;  }, {&#xA;    "state" : "STARTED",&#xA;    "primary" : true,&#xA;    "node" : "1DpvDUf7SKywJrBgQqs9eg",&#xA;    "relocating_node" : null,&#xA;    "shard" : 3,&#xA;    "index" : "osm"&#xA;  } ], [ {&#xA;    "state" : "STARTED",&#xA;    "primary" : false,&#xA;    "node" : "ibpt8lGiS6yDJf4e09RN9Q",&#xA;    "relocating_node" : null,&#xA;    "shard" : 4,&#xA;    "index" : "osm"&#xA;  }, {&#xA;    "state" : "STARTED",&#xA;    "primary" : true,&#xA;    "node" : "FiBYw-v_QfO3nJQfHflf_w",&#xA;    "relocating_node" : null,&#xA;    "shard" : 4,&#xA;    "index" : "osm"&#xA;  } ] ]&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>However the ES_HEAP_SIZE set on the server is half of the ram :</p>&#xA;&#xA;<pre><code>~$ echo $ES_HEAP_SIZE&#xA;7233.0m&#xA;</code></pre>&#xA;&#xA;<p>and the usage is only 5g:</p>&#xA;&#xA;<pre><code>~$ free -g&#xA;             total       used&#xA;Mem:            14          5&#xA;</code></pre>&#xA;&#xA;<p>and if I wait a bit more the node completely leaves the cluster and all the replicas get in initialized state which makes my insertion fail and stop :</p>&#xA;&#xA;<pre><code>{&#xA;    "state" : "INITIALIZING",&#xA;    "primary" : false,&#xA;    "node" : "ibpt8lGiS6yDJf4e09RN9Q",&#xA;    "relocating_node" : null,&#xA;    "shard" : 3,&#xA;    "index" : "osm",&#xA;    "unassigned_info" : {&#xA;      "reason" : "NODE_LEFT",&#xA;      "at" : "2015-10-30T10:53:32.044Z",&#xA;      "details" : "node_left[FiBYw-v_QfO3nJQfHflf_w]"&#xA;    }&#xA;</code></pre>&#xA;&#xA;<p>Conf : In order to speed up the insert I use those parameters on the data nodes elasticsearch configuration</p>&#xA;&#xA;<p>refresh_interval : -1,&#xA;threadpool.bulk.size: 16,&#xA;threadpool.bulk.queue_size: 1000</p>&#xA;&#xA;<p>Why does this happen ? And how can I fix this and get my bulk insert to succeed?&#xA;Do I need more than 50% of the RAM for the maximum heap size?</p>&#xA;&#xA;<p>EDIT : Since it's not good to tweak the elasticsearch parameters I removed the threadpool parameters and it worked but very slowly. Elasticsearch is not designed to ingest too much data too fast.</p>&#xA;
<p>Remove these settings:</p>&#xA;&#xA;<pre><code>threadpool.bulk.size: 16&#xA;threadpool.bulk.queue_size: 1000&#xA;</code></pre>&#xA;&#xA;<p>Those setting's defaults should be good enough for not overloading the cluster.</p>&#xA;&#xA;<p>And make sure you size properly your bulk indexing process, following the instructions <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html#_how_big_is_too_big" rel="nofollow">here</a>. Depending on the cluster/data the bulk needs to have a certain size. You can't use whatever values you want for those hoping to ingest as much as possible. Every cluster has limitations and you should test yours.</p>&#xA;