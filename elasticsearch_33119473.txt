33119473
Elasticsearch Bulk Write is slow using Scan and Scroll
<p>I am currently running into an issue on which i am really stuck.&#xA;I am trying to work on a problem where I have to output the Elasticsearch documents and write them to csv. The docs range from 50,000 to 5 million.&#xA;I am experience serious performance issues and I get a feeling that I am missing something here.</p>&#xA;&#xA;<p>Right now I have a dataset to 400,000 documents on which I am trying to scan and scroll and which would ultimately be formatted and written to csv. But the time taken to just output is 20 mins!! That is insane.</p>&#xA;&#xA;<p>Here is my script:</p>&#xA;&#xA;<pre><code>import elasticsearch&#xA;import elasticsearch.exceptions &#xA;import elasticsearch.helpers as helpers&#xA;import time&#xA;&#xA;es =  elasticsearch.Elasticsearch(['http://XX.XXX.XX.XXX:9200'],retry_on_timeout=True)&#xA;&#xA;scanResp = helpers.scan(client=es,scroll="5m",index='MyDoc',doc_type='MyDoc',timeout="50m",size=1000)&#xA;&#xA;resp={}&#xA;start_time = time.time()&#xA;for resp in scanResp:&#xA;    data = resp&#xA;    print data.values()[3]&#xA;&#xA;print("--- %s seconds ---" % (time.time() - start_time))&#xA;</code></pre>&#xA;&#xA;<p>I am using a hosted AWS m3.medium server for Elasticsearch.</p>&#xA;&#xA;<p>Can anyone please tell me what I might be doing wrong here?</p>&#xA;
<p>A simple solution to output ES data to CSV is to use Logstash with an <a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-elasticsearch.html" rel="nofollow"><code>elasticsearch</code> input</a> and a <a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-csv.html" rel="nofollow"><code>csv</code> output</a> with the following <code>es2csv.conf</code> config:</p>&#xA;&#xA;<pre><code>input {&#xA;  elasticsearch {&#xA;   host =&gt; "localhost"&#xA;   port =&gt; 9200&#xA;   index =&gt; "MyDoc"&#xA;  }&#xA;}&#xA;filter {&#xA; mutate {&#xA;  remove_field =&gt; [ "@version", "@timestamp" ]&#xA; }&#xA;}&#xA;output {&#xA; csv {&#xA;   fields =&gt; ["field1", "field2", "field3"]  &lt;--- specify the field names you want &#xA;   path =&gt; "/path/to/your/file.csv"&#xA; }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>You can then export your data easily with <code>bin/logstash -f es2csv.conf</code></p>&#xA;