29010826
ElasticSearch "H & R Block" with partial word search
<p>The requirements are to be able to search the following terms : </p>&#xA;&#xA;<ol>&#xA;<li>"H &amp; R" to find "H &amp; R Block".</li>&#xA;</ol>&#xA;&#xA;<p>I have managed to implement this requirement alone using word_delimiter, as mentionned in this answer <a href="https://stackoverflow.com/questions/18223101/elasticsearch-tokenize-hr-blocks-as-h-r-hr-blocks">elasticsearch tokenize &quot;H&amp;R Blocks&quot; as &quot;H&quot;, &quot;R&quot;, &quot;H&amp;R&quot;, &quot;Blocks&quot;</a></p>&#xA;&#xA;<p>Using ruby code :</p>&#xA;&#xA;<pre><code>{&#xA;  char_filter: {&#xA;    strip_punctuation: { type: "mapping", mappings: [".=&gt;", ",=&gt;", "!=&gt;", "?=&gt;"] },&#xA;  },&#xA;  filter: {&#xA;    my_splitter: { &#xA;      type: "word_delimiter", &#xA;      preserve_original: true &#xA;    }&#xA;  },&#xA;  analyzer: {&#xA;    my_analyzer {&#xA;      char_filter: %w[strip_punctuation],&#xA;      type: "custom",&#xA;      tokenizer: "whitespace",&#xA;      filter: %w[lowercase asciifolding my_splitter]&#xA;    }&#xA;  }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>But also, in the same query, we want autocomplete functionality or partial word matching, so </p>&#xA;&#xA;<ol start="2">&#xA;<li>"Ser", "Serv", "Servi", "Servic" and "Service" all find "Service" and "Services".</li>&#xA;</ol>&#xA;&#xA;<p>I have managed to implement this requirement alone, using ngram.</p>&#xA;&#xA;<pre><code>{&#xA;  char_filter: {&#xA;    strip_punctuation: { type: "mapping", mappings: [".=&gt;", ",=&gt;", "!=&gt;", "?=&gt;"] }&#xA;  },&#xA;  analyzer: {&#xA;    my_analyzer: {&#xA;      char_filter: %w[strip_punctuation],&#xA;      tokenizer: "my_ngram",&#xA;      filter: %w[lowercase asciifolding]&#xA;    }&#xA;  },&#xA;  tokenizer: {&#xA;    my_ngram: {&#xA;      type: "nGram",&#xA;      min_gram: "3",&#xA;      max_gram: "10",&#xA;      token_chars: %w[letter digit]&#xA;    }&#xA;  } &#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I just can't manage to implement them together. When I use ngram, short words are ignored so "H &amp; R" is left out. When I use word_delimiter, partial word searches stop working. Below, my latest attempt at merging both requirements, it results in supporting partial word searches but not "H &amp; R".</p>&#xA;&#xA;<pre><code>{&#xA;  char_filter: {&#xA;    strip_punctuation: { type: "mapping", mappings: [".=&gt;", ",=&gt;", "!=&gt;", "?=&gt;"] }&#xA;  },&#xA;  filter: {&#xA;    my_splitter: {&#xA;      type: "word_delimiter",&#xA;      preserve_original: true&#xA;    }&#xA;  },&#xA;  analyzer: {&#xA;    my_analyzer: {&#xA;      char_filter: %w[strip_punctuation],&#xA;      type: "custom",&#xA;      tokenizer: "my_tokenizer",&#xA;      filter: %w[lowercase asciifolding my_splitter]&#xA;    }&#xA;  },&#xA;  tokenizer: {&#xA;    my_tokenizer: {&#xA;      type: "nGram",&#xA;      min_gram: "3",&#xA;      max_gram: "10",&#xA;      token_chars: %w[letter digit]&#xA;    }&#xA;  } &#xA;}&#xA;</code></pre>&#xA;
<p>You can use <a href="http://www.elastic.co/guide/en/elasticsearch/reference/current/_multi_fields.html" rel="nofollow"><code>multi_field</code></a> from your mapping to index the same field in multiple ways. You can use your full text search with custom tokenizer on the default field, and create a special indexing for your autocompletion needs. </p>&#xA;&#xA;<pre><code>"title": {&#xA;    "type": "string",&#xA;    "fields": {&#xA;        "raw":   { "type": "string", "index": "not_analyzed" }&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Your query will need to be slightly different when performing the autocomplete as the field will be <code>title.raw</code> instead of just <code>title</code>.</p>&#xA;&#xA;<p>Once the field is indexed in all the ways that make sense for your query, you can query the index using a boolean "should" query, matching the tokenized version and the word start query. It is likely that a larger boost should be provided to the first query matching complete words to get the direct hits on top.</p>&#xA;