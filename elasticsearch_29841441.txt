29841441
Elasticsearch Bulk indexing creates a lot of disk read OPS
<p>I've created an index I use for logging.</p>&#xA;&#xA;<p>This means there are mostly writes, and some searches once in a while.&#xA;In the phase of the first loading, I'm using several clients to concurrently index documents using the bulk API.</p>&#xA;&#xA;<p>At first, indexing takes 200 ms for a bulk of 5000 documents.&#xA;As time goes by, the indexing time increases, and gets to 1000-4500 ms.</p>&#xA;&#xA;<p>I am using an EC2 c3.8xl machine with 32 cores, and 60 GB of memory, with an IO provisioned volume set to 7000 IOPS.</p>&#xA;&#xA;<p>I have 10 shards, no replicas, all on the same machine.&#xA;ATM, there are some 1.5 billion records in the index.</p>&#xA;&#xA;<p>Looking at the metrics, I see that the CPU and memory are fine, the write IOPS are at 300, but the read IOPS have slowly gone up and got to 7000.</p>&#xA;&#xA;<p>How come I'm only indexing, but most of the IOPS are read?</p>&#xA;&#xA;<p>My settings are:</p>&#xA;&#xA;<pre><code>threadpool.bulk.type: fixed&#xA;threadpool.bulk.size: 32                 # availableProcessors&#xA;threadpool.bulk.queue_size: 1000&#xA;&#xA;# Indices settings&#xA;indices.memory.index_buffer_size: 50%&#xA;&#xA;indices.cache.filter.expire: 6h&#xA;&#xA;bootstrap.mlockall: true&#xA;</code></pre>&#xA;&#xA;<p>and I've change the index settings to:</p>&#xA;&#xA;<pre><code>{"index":{"refresh_interval":"60m",&#xA;    "translog":&#xA;        {"flush_threshold_size":"1gb",&#xA;        "flush_threshold_ops":"50000"}&#xA;    }&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>I also tried "refresh_interval":"-1"</p>&#xA;&#xA;<p>Please let me know what else I need to provide if needed (settings, logs, metrics). Here are the node stats:</p>&#xA;&#xA;<pre><code>"_all": {&#xA;&#xA;    "primaries": {&#xA;        "docs": {&#xA;            "count": 1473959582,&#xA;            "deleted": 1376161&#xA;        },&#xA;        "store": {&#xA;            "size_in_bytes": 497545621011,&#xA;            "throttle_time_in_millis": 102780138&#xA;        },&#xA;        "indexing": {&#xA;            "index_total": 416653525,&#xA;            "index_time_in_millis": 679407284,&#xA;            "index_current": 0,&#xA;            "delete_total": 0,&#xA;            "delete_time_in_millis": 0,&#xA;            "delete_current": 0,&#xA;            "noop_update_total": 1,&#xA;            "is_throttled": false,&#xA;            "throttle_time_in_millis": 0&#xA;        },&#xA;        "get": {&#xA;            "total": 2943640,&#xA;            "time_in_millis": 15160148,&#xA;            "exists_total": 1445558,&#xA;            "exists_time_in_millis": 7460238,&#xA;            "missing_total": 1498082,&#xA;            "missing_time_in_millis": 7699910,&#xA;            "current": 0&#xA;        },&#xA;        "search": {&#xA;            "open_contexts": 0,&#xA;            "query_total": 70,&#xA;            "query_time_in_millis": 12238,&#xA;            "query_current": 0,&#xA;            "fetch_total": 2,&#xA;            "fetch_time_in_millis": 23,&#xA;            "fetch_current": 0&#xA;        },&#xA;        "merges": {&#xA;            "current": 0,&#xA;            "current_docs": 0,&#xA;            "current_size_in_bytes": 0,&#xA;            "total": 4184,&#xA;            "total_time_in_millis": 128875711,&#xA;            "total_docs": 1282672895,&#xA;            "total_size_in_bytes": 429203874419&#xA;        },&#xA;        "refresh": {&#xA;            "total": 1930,&#xA;            "total_time_in_millis": 1816632&#xA;        },&#xA;        "flush": {&#xA;            "total": 7774,&#xA;            "total_time_in_millis": 4783754&#xA;        },&#xA;        "warmer": {&#xA;            "current": 0,&#xA;            "total": 23565,&#xA;            "total_time_in_millis": 1792&#xA;        },&#xA;        "filter_cache": {&#xA;            "memory_size_in_bytes": 184938864,&#xA;            "evictions": 0&#xA;        },&#xA;        "id_cache": {&#xA;            "memory_size_in_bytes": 0&#xA;        },&#xA;        "fielddata": {&#xA;            "memory_size_in_bytes": 0,&#xA;            "evictions": 0&#xA;        },&#xA;        "percolate": {&#xA;            "total": 0,&#xA;            "time_in_millis": 0,&#xA;            "current": 0,&#xA;            "memory_size_in_bytes": -1,&#xA;            "memory_size": "-1b",&#xA;            "queries": 0&#xA;        },&#xA;        "completion": {&#xA;            "size_in_bytes": 0&#xA;        },&#xA;        "segments": {&#xA;            "count": 368,&#xA;            "memory_in_bytes": 877782264,&#xA;            "index_writer_memory_in_bytes": 23671280,&#xA;            "index_writer_max_memory_in_bytes": 5368709120,&#xA;            "version_map_memory_in_bytes": 19674480,&#xA;            "fixed_bit_set_memory_in_bytes": 0&#xA;        },&#xA;        "translog": {&#xA;            "operations": 213819,&#xA;            "size_in_bytes": 19598986&#xA;        },&#xA;        "suggest": {&#xA;            "total": 0,&#xA;            "time_in_millis": 0,&#xA;            "current": 0&#xA;        },&#xA;        "query_cache": {&#xA;            "memory_size_in_bytes": 0,&#xA;            "evictions": 0,&#xA;            "hit_count": 0,&#xA;            "miss_count": 0&#xA;        },&#xA;        "recovery": {&#xA;            "current_as_source": 0,&#xA;            "current_as_target": 0,&#xA;            "throttle_time_in_millis": 0&#xA;        }&#xA;    }&#xA;</code></pre>&#xA;
<p>When you index a document it has to go look for that ID to know if it needs to mark an older version of a document as deleted. As your index grows the number of segments naturally goes up too. Thus, ES must perform more seeks to find a given ID. I suspect if you did an optimization on the index that you would see the number of disk reads decrease, at least for a while. </p>&#xA;&#xA;<p>You may have to tweak your merge policy to more agressivly merge the number of segments as you insert documents, or schedule optimizations during non peak times. </p>&#xA;&#xA;<p>update: As an after thought, 10 shards for a single index on a single node seems overkill. Unless you have tested other configurations already or plan to add more noes, I would advise dropping that down; perhaps as low as 1 or 2.</p>&#xA;