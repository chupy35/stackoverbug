33836796
Understanding elasticsearch logs: Bad news present, but not good news
<p>Elasticsearch 1.7.2 on CentOS</p>&#xA;&#xA;<p>Issue: Cluster went from green status to off green (not sure yellow or red, we just alert when status &lt;> green), and then back to green by itself.</p>&#xA;&#xA;<p>Elasticsearch log:</p>&#xA;&#xA;<pre><code>[2015-11-20 18:11:50,750][WARN ][cluster.action.shard     ] [node-6a] [index_v6][1] received shard failed for [index_v6][1], node[rNAaZCFtTv6cG1eMP30b9g], [R], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-11-20T18:11:42.859Z], details[shard failure [engine failure, reason [out of memory (source: [index])]][OutOfMemoryError[Java heap space]]]], indexUUID [4lNodBboRoep64nAfxXbeQ], reason [shard failure [failed to create shard][IndexShardCreationException[[index_v6][1] failed to create shard]; nested: LockObtainFailedException[Can't lock shard [index_v6][1], timed out after 5000ms]; ]]&#xA;[2015-11-20 18:14:34,074][WARN ][cluster.action.shard     ] [node-6a] [index_v6][2] received shard failed for [index_v6][2], node[ur-Fm7iCSYSemcHWLTgIVA], [R], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-11-20T18:03:56.528Z], details[shard failure [engine failure, reason [already closed by tragic event]][OutOfMemoryError[Java heap space]]]], indexUUID [4lNodBboRoep64nAfxXbeQ], reason [shard failure [failed recovery][RecoveryFailedException[[index_v6][2]: Recovery failed from [node-6a][rNAaZCFtTv6cG1eMP30b9g][elastic-search-6a][inet[/10.#########:9300]]{master=true} into [node-6b][ur-Fm7iCSYSemcHWLTgIVA][elastic-search-6b][inet[/10.######:9300]]{master=true}]; nested: RemoteTransportException[[node-6a][inet[/10.#########:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[index_v6][2] Phase[1] Execution failed]; nested: RecoverFilesRecoveryException[[index_v6][2] Failed to transfer [146] files with total size of [9.9gb]]; nested: SendRequestTransportException[[node-6b][inet[/10.######:9300]][internal:index/shard/recovery/file_chunk]]; nested: OutOfMemoryError[Java heap space]; ]]&#xA;</code></pre>&#xA;&#xA;<p>We understand that we hit the java heap memory limit. We are used to that (unfortunately).</p>&#xA;&#xA;<p>What we do not understand:</p>&#xA;&#xA;<p>1) The cluster went back to green, but we see nothing in the logs to help us understand when or why.</p>&#xA;&#xA;<p>2) We thought these config values might help:</p>&#xA;&#xA;<pre><code>indices.fielddata.cache.size:  75%&#xA;indices.breaker.fielddata.limit: 70%&#xA;</code></pre>&#xA;&#xA;<p>But they seem to make no difference.</p>&#xA;&#xA;<p>(We are still in process of moving to doc values, etc.)</p>&#xA;