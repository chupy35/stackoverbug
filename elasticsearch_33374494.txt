33374494
elasticsearch: Did I lose data when two of my three nodes went down?
<p>Elasticsearch 1.7.2 on CentOS</p>&#xA;&#xA;<p>The question: When my nodes B and C went down, did I lose data?</p>&#xA;&#xA;<p>3 node cluster: Nodes: A, B, C</p>&#xA;&#xA;<p>A is master (was set up first, worked out that way). Relevant config (on all nodes, however what happened was the B lost network access and went down, and it turned out that C incorrectly was set to number_of_replicas: 1)</p>&#xA;&#xA;<pre><code>node.master: true&#xA;node.data: true&#xA;index.number_of_shards: 5&#xA;index.number_of_replicas: 2&#xA;</code></pre>&#xA;&#xA;<p>On A, while those other two nodes were down, I notice that the "unassigned_shards" is 6. Since my shard count is 5, that implies to me that I have a problem:</p>&#xA;&#xA;<pre><code># curl -XGET http://localhost:9200/_cluster/health?pretty=true&#xA;{&#xA;  "cluster_name" : "elasticsearch-PROD-prod",&#xA;  "status" : "red",&#xA;  "timed_out" : false,&#xA;  "number_of_nodes" : 1,&#xA;  "number_of_data_nodes" : 1,&#xA;  "active_primary_shards" : 4,&#xA;  "active_shards" : 4,&#xA;  "relocating_shards" : 0,&#xA;  "initializing_shards" : 0,&#xA;  "unassigned_shards" : 6,&#xA;  "delayed_unassigned_shards" : 0,&#xA;  "number_of_pending_tasks" : 0,&#xA;  "number_of_in_flight_fetch" : 0&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>Sure enough, on the shard list below, there is a primary shard (#1) that is UNASSIGNED</p>&#xA;&#xA;<pre><code># curl -XGET http://localhost:9200/_cat/shards&#xA;index_v3_PROD 4 p STARTED    22578283 12.7gb 10.208.131.56 PROD-node-3a&#xA;index_v3_PROD 4 r UNASSIGNED                                   &#xA;index_v3_PROD 0 p STARTED    22572884 12.7gb 10.208.131.56 PROD-node-3a&#xA;index_v3_PROD 0 r UNASSIGNED                                   &#xA;index_v3_PROD 3 p STARTED    22579159 12.8gb 10.208.131.56 PROD-node-3a&#xA;index_v3_PROD 3 r UNASSIGNED                                   &#xA;index_v3_PROD 1 p UNASSIGNED                                   &#xA;index_v3_PROD 1 r UNASSIGNED                                   &#xA;index_v3_PROD 2 p STARTED    22580877 12.7gb 10.208.131.56 PROD-node-3a&#xA;index_v3_PROD 2 r UNASSIGNED                                   &#xA;</code></pre>&#xA;&#xA;<p><strong>Notice above that shard 1 is "p" and is UNASSIGNED. This looks scary to me!</strong></p>&#xA;&#xA;<p>I then used a reroute command to assign it over to A, which it did.</p>&#xA;&#xA;<pre><code>curl -XPOST 'localhost:9200/_cluster/reroute' -d '{"commands" : [ {&#xA;              "allocate" : {&#xA;                  "index" : "index_v3_PROD", &#xA;                  "shard" : 1, &#xA;                  "node" : "PROD-node-3a", &#xA;                  "allow_primary" : true&#xA;              }&#xA;            }&#xA;        ]&#xA;    }'&#xA;</code></pre>&#xA;&#xA;<p>But shard 1 started at a very small size and then kind of grew (I think from new data being sent to ES). I have a strong feeling that shard 1 data was lost.</p>&#xA;&#xA;<p>Can someone confirm whether shard 1 data looks suspect/lost (or not)?</p>&#xA;
<p>Andrei posted this as a comment, not as an answer, so I will:</p>&#xA;&#xA;<p>Yes, data was lost. </p>&#xA;&#xA;<p>A reroute command for a primary shard that has "allow_primary": true and that shard is not available will start the shard from scratch, empty. You should have made everything possible to bring that node back in the cluster.</p>&#xA;&#xA;<hr>&#xA;&#xA;<p>The footnote here is: We did not know what to do to bring back a cluster node. The other 2 nodes showed this status:</p>&#xA;&#xA;<pre><code># curl -XGET http://localhost:9200/_cluster/health?pretty=true&#xA;{&#xA;  "error" : "MasterNotDiscoveredException[waited for [30s]]",&#xA;  "status" : 503&#xA;}&#xA;</code></pre>&#xA;&#xA;<p>We could not find any diag steps to use, and the logs showed us nothing useful. Two nodes both reported this issue (there are 3 total, so the only copy of the needed shard was on one of those two).</p>&#xA;&#xA;<p>We verified all-way network communication, and rebooted the other nodes, but they did not attach to the cluster.</p>&#xA;&#xA;<p>Ultimately, we set up a fresh 3 node cluster, and are being better about ensuring that every node has every shard present, so that the cluster could withstand losing 2 nodes.</p>&#xA;